---
title: "Audit analysis report"
date: "v1, generated on `r format(Sys.time(), '%d-%b-%Y')`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{r global_options, include=FALSE}
knitr::opts_chunk$set(warning=FALSE, message=FALSE)
```

```{r, include=FALSE}
if (!require("pacman")) install.packages("pacman")
pacman::p_load(tidyverse, DT, readxl, anytime, cluster, data.table, parallel)
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))

source("./utils.R")
dir.create("output", showWarnings = F, recursive = T)
#------------------------------------------------------------------------------------
# INPUT THE FOLLOWING PARAMETERS FOR EACH ASSESSMENT
#------------------------------------------------------------------------------------
# specify assessment name
assessment.name <- "DRC MSNA"
# specify column names of UUID and enumerator-code as they are in the survey dataset
uuid.column.name <- "uuid"
enumerator.column.name <- "enum_id"
#------------------------------------------------------------------------------------

# set up country
country <- "DRC"

# set up enumerator column / amdin1
col_group <- "admin2"

# load tool (copy paste it from the Kobo tool folder)
tool.path <- "../../Data/REACH_DRC2404_MSNA2024_Clean-Data.xlsx"
tool <- read_excel(tool.path, "survey")
choices <- read_excel(tool.path, "choices", guess_max = 23000) %>% rename("label::french"=label)

tool.combined <- combine_tool(survey = tool, responses = choices, label_col = "label::french")
col.sm <- tool.combined %>% filter(q.type=="select_multiple") %>% pull(name) %>% unique
col.so <- tool.combined %>% filter(q.type=="select_one") %>% pull(name) %>% unique
col.int <- tool %>% filter(type=="integer") %>% pull(name) %>% unique
col.text <- tool %>% filter(type=="text") %>% pull(name) %>% unique

# load all audit files
directory.audits <- "C:/Users/raphael.bacot/OneDrive - ACTED/A&I/MSNA DRC Audit/"

## if written audit.cs exists, fread instead of loading all audit files
if(file.exists("output/audit.csv")){
  audit <- fread("output/audit.csv")
} else {
  audit <- load.audit.files(directory.audits)
  fwrite(audit, "output/audit.csv", row.names=F)
}

# load survey data
directory.survey.data <- "../../Data/REACH_DRC2404_MSNA2024_Clean-Data.xlsx"
survey.data <- read_excel(directory.survey.data, sheet="hh data")

### check that there are no uuid in audit that are not present in the survey.data
# audit <- audit %>% filter(uuid %in% survey.data[[uuid.column.name]])

```

# {.tabset}

## Report

### Assessment details

Assessment name: *`r assessment.name`*

Number of audit logs loaded: *`r length(unique(audit$uuid))`*

---

### Summary of paradata for each survey

```{r}
## for whole survey
summary.general <- get.summary.general(audit) %>% left_join(survey.data, by="uuid") 
summary.general.hesper <- get.summary.general(audit,filter.pattern = "^hesper_") %>% left_join(survey.data, by="uuid") 

summary.general.pivot <- summary.general %>% 
  pivot_wider(names_from = event, values_from = n, values_fill = 0) %>% 
  mutate(survey.response.time=base::round(survey.response.time/60, 1),
         survey.duration=base::round(survey.duration/60, 1)) %>% 
  select(-c(question, num.unique.nodes))
fwrite(summary.general.pivot, "output/survey.general.csv", row.names=F)

summary.general.pivot.hesper <- summary.general.hesper %>% 
  pivot_wider(data = ., names_from = event, values_from = n, values_fill = 0) %>% 
  mutate(survey.response.time=base::round(survey.response.time/60, 1),
         survey.duration=base::round(survey.duration/60, 1)) %>% 
  select(-c(question, num.unique.nodes))
# fwrite(summary.general.pivot.hesper, "output/survey.general.hesper.csv", row.names=F)

# datatable(summary.general.pivot, 
#           caption="List of all surveys with summary of audit events. Survey duration and response time are in minutes.", 
#           options = list(scrollX=TRUE, pageLength = 5))
```

---

### Average statistics per admin 2

```{r}
admin1_name.view <- summary.general %>% 
  group_by(!!sym(col_group)) %>% 
  mutate(num.surveys=length(unique(uuid))) %>% 
  group_by(!!sym(col_group), event) %>% summarise(num.surveys=num.surveys[1], avg=base::round(sum(n)/num.surveys, 1)) %>% 
  pivot_wider(names_from="event", values_from="avg", values_fill=0) %>% 
  mutate(survey.response.time=base::round(survey.response.time/60, 1),
         survey.duration=base::round(survey.duration/60, 1)) %>% 
  arrange(-num.surveys)

cols <- c("num.surveys", "form.resume", "jump", 
          "num.unique.questions", "avg.edits.per.question",
          "survey.response.time", "survey.duration")
cols.intersect <- intersect(cols, colnames(admin1_name.view))

datatable(admin1_name.view %>% select(all_of(col_group), all_of(cols.intersect)),
          caption="Number of conducted surveys, average number of events, average survey response time and duration in minutes per admin1", 
          options = list(scrollX=TRUE, pageLength = 10))
```

### Average statistics per enumerator

```{r}
## same as above but use enumerator.column.name / !!sym(enumerator.column.name) instead of admin1_name
enum.view <- summary.general %>% 
  group_by(!!sym(enumerator.column.name)) %>% 
  mutate(num.surveys=length(unique(uuid))) %>% 
  group_by(!!sym(enumerator.column.name), event) %>% summarise(num.surveys=num.surveys[1], avg=base::round(sum(n)/num.surveys, 1)) %>% 
  pivot_wider(names_from="event", values_from="avg", values_fill=0) %>% 
  mutate(survey.response.time=base::round(survey.response.time/60, 1),
         survey.duration=base::round(survey.duration/60, 1)) %>% 
  arrange(-num.surveys)

datatable(enum.view %>% select(all_of(enumerator.column.name), all_of(cols.intersect)),
          caption="Number of conducted surveys, average number of events, average survey response time and duration in minutes per enumerator", 
          options = list(scrollX=TRUE, pageLength = 10))
```

---

### Average question response time per admin 2

```{r, fig.height=10, fig.width=10}
stats.question.uuid <- audit %>% filter(node!="") %>%
  group_by(uuid, question) %>% summarise(duration=sum(duration, na.rm=TRUE)) %>% 
  left_join(survey.data, by="uuid")

stats.question <- stats.question.uuid %>% 
  group_by(!!sym(col_group)) %>% mutate(num.surveys=length(unique(uuid))) %>% 
  group_by(!!sym(col_group), question) %>% summarise(num.surveys=num.surveys[1], avg.response.time=base::round(mean(duration), 1)) %>% 
  filter(!question %in% col_group) %>%
  pivot_wider(names_from="question", values_from="avg.response.time", values_fill=0) %>% 
  arrange(-num.surveys)

## write stats.question in output
# fwrite(stats.question, "output/question.response.time.csv", row.names=F)
cols.sorted <- c(col_group, "num.surveys", sort(colnames(stats.question)[3:length(colnames(stats.question))]))
stats.question.hesper <- stats.question %>% select(all_of(str_subset(cols.sorted, "hesper")))

datatable(stats.question %>% 
            select(all_of(str_subset(cols.sorted, "hesper"))),
          caption="Average question response time in seconds per admin 2", 
          options = list(scrollX=TRUE, pageLength = 10))
```

### Average question response time per enumerator

```{r}
## same for enumerator using enumerator.column.name
stats.question <- stats.question.uuid %>% 
  group_by(!!sym(enumerator.column.name)) %>% mutate(num.surveys=length(unique(uuid))) %>% 
  group_by(!!sym(enumerator.column.name), question) %>% summarise(num.surveys=num.surveys[1], avg.response.time=base::round(mean(duration), 1)) %>% 
  filter(!question %in% enumerator.column.name) %>%
  pivot_wider(names_from="question", values_from="avg.response.time", values_fill=0) %>%
  arrange(-num.surveys)

datatable(stats.question %>% 
            select(all_of(str_subset(cols.sorted, "hesper"))),
          caption="Average question response time in seconds per enumerator", 
          options = list(scrollX=TRUE, pageLength = 10))
```

---

### Anomaly score

In this section we calculate an anomaly score between 0 and 1 for each admin 2 which can help to detect irregular behaviours. Scores above 0.2 should be further investigated. The anomaly score is calculated as the average of two components:

* The *deviation.score* [0-1] indicates how much the average response time of the surveys from one admin 2 deviates from the overall median response time of all surveys. A low score indicates that the average response time for the admin 2 is close to the overall median response time. While, a high score indicates that the average response time for the admin 2 is either much higher or much lower than the overall median response time. The deviation.score is calculated for each question  and then averaged.

* The *gower.score* [0-1] indicates how much the admin 2 interaction with Kobo deviates from a regular behaviour. Particular focus is on the number of occurences of *form.resume*, *jump*, *constraint.error* and on the *survey.response.time*, *num.unique.questions*, and *avg.edits.per.question*. Once again, a low value indicates a regular interaction while a high value indicates an irregular one (e.g. high number of jumps or avg.edits.per.question). The gower.score is calculated for each survey and then averaged for each admin 2.

```{r}
## DEVIATION FROM MEDIAN RESPONSE TIME
get.anomaly.score <- function(distr, value){
  exp.factor <- 5
  quantile <- sum(sort(distr) < value)/length(distr)
  distance.from.median <- abs(quantile-0.5)
  score <- distance.from.median^exp.factor / 0.5^exp.factor
  return(score)
}

if (file.exists("output/enum.anomaly.csv")) {
  stats.enum.final <- fread("output/enum.anomaly.csv")
} else {
  stats.question.uuid <- audit %>% filter(node!="") %>%
  group_by(uuid, question) %>% summarise(duration=sum(duration, na.rm=TRUE)) %>% 
  left_join(survey.data, by="uuid")

stats.admin1_name <- stats.question.uuid %>% 
  group_by(question) %>% mutate(distr=list(duration)) %>% 
  group_by(!!sym(enumerator.column.name)) %>% mutate(num.surveys=length(unique(uuid))) %>% 
  group_by(!!sym(enumerator.column.name), question) %>% summarise(num.surveys=num.surveys[1],
                                         mean.rt=mean(duration),
                                         distr=distr[1])
stats.admin1_name[["anomaly.score"]] <- apply(stats.admin1_name, 1, function(x) get.anomaly.score(x$distr, x$mean.rt))
stats.admin1_name <- stats.admin1_name %>% 
  group_by(!!sym(enumerator.column.name)) %>% summarise(num.surveys=num.surveys[1], deviation.score=base::round(mean(anomaly.score), 2))

## DEVIATON FROM REGULAR KOBO INTERACTIONS
cols <- c("end.screen", "form.exit", "form.finalize", "form.save", 
          "form.start", "form.resume", "jump", "constraint.error", 
          "survey.duration", "survey.response.time", "num.unique.questions", "avg.edits.per.question")
is.cols.present <- cols %in% colnames(summary.general.pivot)
cols <- cols[is.cols.present]
weights <- c(1,1,1,1, 1,3,3,3, 1,3,3,3)
weights <- weights[is.cols.present]

s <- summary.general.pivot %>% pivot_longer(cols=all_of(cols), names_to="event", values_to="n") %>% 
  group_by(event) %>% summarise(uuid="MEDIAN", !!sym(enumerator.column.name):="MEDIAN", median=median(n)) %>% 
  pivot_wider(data = ., names_from = event, values_from = median, values_fill = 0)

all <- rbind(select(summary.general.pivot, c("uuid", enumerator.column.name, cols)), s)
ref.idx <- which(all$uuid=="MEDIAN")

gower_dist <- daisy(all[, -(1:2)], metric = "gower", weights=weights)
gower_mat <- as.matrix(gower_dist)

all[["gower.dist"]] <- gower_mat[ref.idx, ]

all.admin1_name <- all %>% filter(!!sym(enumerator.column.name)!="MEDIAN") %>% group_by(!!sym(enumerator.column.name)) %>% summarise(gower.score = base::round(mean(gower.dist), 2))

## COMBINED ANOMALY SCORE
stats.enum.final <- left_join(stats.admin1_name, all.admin1_name, by=enumerator.column.name) %>% mutate(anomaly.score=base::round((deviation.score+gower.score)/2, 2))
fwrite(stats.enum.final, "output/enum.anomaly.csv", row.names = F)
}

datatable(arrange(stats.enum.final, -anomaly.score),
          caption="Anomaly score based on deviation from median response time and regular Kobo interaction", 
          options = list(scrollX=TRUE, pageLength = 10))

```


---

### Question response time from all surveys

```{r}
stats.question.uuid <- audit %>% filter(node!="") %>%
  group_by(uuid, question) %>% summarise(response.time=sum(duration, na.rm=TRUE)) %>% 
  group_by(question) %>% summarise(num.responses=n(),
                                   median.response.time=base::round(median(response.time, na.rm=TRUE), 2),
                                   min.response.time=base::round(min(response.time, na.rm=TRUE), 2),
                                   max.response.time=base::round(max(response.time, na.rm=TRUE), 2),
                                   ## add quartiles
                                   q25=base::round(quantile(response.time, 0.25, na.rm=TRUE), 2),
                                   q75=base::round(quantile(response.time, 0.75, na.rm=TRUE), 2))
datatable(stats.question.uuid,
          caption="Median, minimum and maximum values of question response time in seconds", 
          options = list(scrollX=TRUE, pageLength = 10))
fwrite(stats.question.uuid, "output/stats.question.uuid.csv", row.names = F)
```

---

### Histogram of survey response time

```{r, fig.height = 4, fig.width = 10, echo=FALSE}
# ggplot(summary.general.pivot, aes(x=survey.response.time)) +
#   geom_histogram(bins=5) +
#   xlab("Survey response time [minutes]") + ylab("Number of surveys") + theme_minimal()
plot <- ggplot(summary.general.pivot, aes(x=survey.response.time)) +
  geom_histogram(bins=base::round(max(summary.general.pivot$survey.response.time)/5,0), fill="#0067A9") +
  geom_vline(aes(xintercept=median(survey.response.time, na.rm=T))) +
  scale_x_continuous(limits=c(0, 250))+
  xlab("Survey response time [minutes]") + ylab("Number of surveys") + theme_minimal()
plot + geom_text(aes(x=median(survey.response.time, na.rm=T) + 20, y=3, label=paste0("median: ", median(survey.response.time, na.rm=T))))
plot + facet_wrap(as.formula(paste("~", col_group)))
```

---

### Histogram of HESPER block response time

```{r, fig.height = 4, fig.width = 10, echo=FALSE}
plot <- ggplot(summary.general.pivot.hesper, aes(x=survey.response.time)) +
  geom_histogram(bins=base::round(max(summary.general.pivot.hesper$survey.response.time)/2,0), fill="#0067A9") +
  geom_vline(aes(xintercept=median(survey.response.time, na.rm=T))) + scale_x_continuous(limits=c(0, 20))+
  geom_text(aes(x=median(survey.response.time, na.rm=T) + 20, y=3, label=paste0("median: ", median(survey.response.time, na.rm=T)))) +
  xlab("Hesper survey block response time [minutes]") + ylab("Number of surveys") + theme_minimal()
plot
plot + facet_wrap(as.formula(paste("~", col_group)))
```

---

### Survey start time distribution

**Warning:** contrary to the response time, that does not depend on the absolute device time since it is calculated as the elapsed time between 2 events, the start time of the survey is the actual timestamp reported by the device for the event *form.start*. Thus, it could be inaccurate if the device time is not set correctly. The following histograms could be affected by this fact.

```{r}
a <- audit %>% filter(event=="form.start") %>% 
  mutate(date=anytime(start/1000, tz="Asia/Damascus")) %>%
  select(start, date) %>% 
  mutate(time=hms::as_hms(as.numeric(str_sub(date, 12, 13))*3600 + 
                            as.numeric(str_sub(date, 15, 16))*60 +
                            as.numeric(str_sub(date, 18, 19))))
```

#### By date and time

```{r, fig.height = 4, fig.width = 10, echo=FALSE}
ggplot(a, aes(x=date)) + geom_histogram(bins=50, fill="#0067A9") + ylab("Number of surveys")+ theme_minimal()
```

#### By time

```{r, fig.height = 4, fig.width = 10, echo=FALSE}
ggplot(a, aes(x=time)) + geom_histogram(bins=50, fill="#0067A9") + ylab("Number of surveys") + theme_minimal()
```

### Ad-hoc definitions

* survey.duration = time from *form start* to the last *form exit* (it can be days later in some cases).

* survey.response.time = sum of the time spent on the screen showing any of the questions (events: *question* and *group questions*).

* num.unique.questions = number of unique questions that were shown on the screen during the survey.

* avg.edits.per.question = average number of times a question was opened during the survey. It is calculated as the ratio between the total number of questions opened and the number of unique questions.